{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![logo](https://github.com/donatellacea/DL_tutorials/blob/main/notebooks/figures/1128-191-max.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explainability of Random Forests\n",
    "\n",
    "In this Notebook we will show you different methods that can be used for interpreting Random Forest models. We will demonstrate you how to apply those methods and how to interpret the results.\n",
    "\n",
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Colab environment\n",
    "\n",
    "If you installed the packages and requirments on your own machine, you can skip this section and start from the import section.\n",
    "Otherwise you can follow and execute the tutorial on your browser. In order to start working on the notebook, click on the following button, this will open this page in the Colab environment and you will be able to execute the code on your own.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/HelmholtzAI-Consultants-Munich/Zero2Hero---Introduction-to-XAI/blob/master/xai-FGC/tutorial_FGC.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you are visualizing the notebook in Colab, run the next cell to install the packages we will use.\n",
    "There are few things you should follow in order to properly set the notebook up:\n",
    "\n",
    "1. Warning: This notebook was not authored by Google. *Click* on 'Run anyway'.\n",
    "2. When the installation commands are done, there might be \"Restart runtime\" button at the end of the output. Please, *click* it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fgclustering==0.3\n",
    "!pip install matplotlib==3.4.3\n",
    "!pip install palmerpenguins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By running the next cell you are going to create a folder in your Google Drive. All the files for this tutorial will be uploaded to this folder. After the first execution you might receive some warning and notification, please follow these instructions:\n",
    "1. Permit this notebook to access your Google Drive files? *Click* on 'Yes', and select your account.\n",
    "2. Google Drive for desktop wants to access your Google Account. *Click* on 'Allow'.\n",
    "\n",
    "At this point, a folder has been created and you can navigate it through the lefthand panel in Colab, you might also have received an email that informs you about the access on your Google Drive. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a folder in your Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd drive/MyDrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't run this cell if you already cloned the repo in the first part of the tutorial\n",
    "!git clone https://github.com/HelmholtzAI-Consultants-Munich/Zero2Hero---Introduction-to-XAI.git  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd Zero2Hero---Introduction-to-XAI/xai-FGC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the required packages\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from palmerpenguins import load_penguins\n",
    "\n",
    "from fgclustering import FgClustering\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils.utils import * \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Palmer Pinguins Dataset: Data Pre-Processing and Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this course, we will work with the **Palmer penguins dataset**, containing the information on 3 different species of penguins - Adelie, Chinstrap, and Gentoo - which were observed in the Palmer Archipelago near Palmer Station, Antarctica. The dataset consist of a total of 344 penguings, together with their size measurements, clutch observations, and blood isotope ratios. Our goal is to predict the species of Palmer penguins and find out the major differences among them.\n",
    "\n",
    "<center><img src=\"./figures/penguins.png\" width=\"500\" /></center>\n",
    "\n",
    "<font size=1> Source:\\\n",
    "https://pypi.org/project/palmerpenguins/#description \\\n",
    "https://allisonhorst.github.io/palmerpenguins/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "penguins = load_penguins()\n",
    "\n",
    "# Inspect the data\n",
    "penguins.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The focus of this notebook is on the interpretation of Random Forest models and not on the data pre-processing or model training part. If you want to learn more about data exploration and each step in the data pre-processing and model trainng pipeline, you can have a look the supplemental notebook [*FGC_supplement.ipynb*](./FGC_supplement.ipynb), which gives a detailed description for each of those steps. Here we briefly list the steps that are done, prepare the data and train the Random Forest model on the Palmer pinguins dataset. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start training the model, we need to do some preprocessing of our dataset. First, we need to take care of the missing values. In this example, we will apply the most common approach and simply omit those cases with the missing data and analyse the remaining data. In addition, categorical features need to be encoded, i.e. turned into numerical data. Here, we will use a simple Label encoding for the categorical features and for the target variable, which will transform the categorical feature values into unique integer values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy data and save original data in penguins variable\n",
    "data_penguins = pd.DataFrame(penguins.copy())\n",
    "\n",
    "# Remove rows with missing values\n",
    "data_penguins.dropna(inplace=True)\n",
    "\n",
    "# Transform the target variable (Species) and the two categorical features (Sex, Island) with LabelEncoder\n",
    "le1 = preprocessing.LabelEncoder()\n",
    "data_penguins.species = le1.fit_transform(data_penguins.species)\n",
    "\n",
    "le2 = preprocessing.LabelEncoder()\n",
    "data_penguins.sex = le2.fit_transform(data_penguins.sex)\n",
    "\n",
    "le3 = preprocessing.LabelEncoder()\n",
    "data_penguins.island = le3.fit_transform(data_penguins.island)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to train our Random Forest model! First, we define a small grid of hyperparameters that is used for model optimization. For demonstration pupose, we define a rather small grid of hyperparameters. Then, we define an instance of the RandomForestClassifier and run the GridSearchCV with the 5-fold cross validation to tune the model on the pre-defined set of hyperparameters. The model with the best hyperparameters is saved as the _best_estimator__ in the GridSearchCV instance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid of hyperparameters \n",
    "hyper_grid_classifier = {'n_estimators': [100, 1000], \n",
    "            'max_depth': [2, 5, 10], \n",
    "            'max_samples': [0.8],\n",
    "            'criterion': ['gini', 'entropy'],\n",
    "            'max_features': ['sqrt','log2']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Random Forest instance from sklearn requires a separate input of feature matrix and target values. \n",
    "# Hence, we will first separate the target and feature columns. \n",
    "X_penguins = data_penguins.loc[:, data_penguins.columns != 'species']\n",
    "y_penguins = data_penguins.species\n",
    "\n",
    "# Define a classifier. We set the oob_score = True, as OOB is a good approximation of the test set score\n",
    "classifier = RandomForestClassifier(oob_score=True, random_state=42, n_jobs=1)\n",
    "\n",
    "# Define a grid search with 5-fold CV and fit \n",
    "gridsearch_classifier = GridSearchCV(classifier, hyper_grid_classifier, cv=5, verbose=1)\n",
    "gridsearch_classifier.fit(X_penguins, y_penguins)\n",
    "\n",
    "# Take the best estimator\n",
    "rf = gridsearch_classifier.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have trouble pre-processing the data or training the Random Forest model, you can load the pre-processed dataset and pre-trained model that we prepared for you by uncommenting and running the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "#data_penguins = pd.read_csv('./data/data_penguins_processed.csv', index_col=0)\n",
    "\n",
    "# Separate the target and feature columns\n",
    "#X_penguins = data_penguins.loc[:, data_penguins.columns != 'species']\n",
    "#y_penguins = data_penguins.species\n",
    "\n",
    "# Load the model\n",
    "#rf = joblib.load(open('./models/random_forest_penguins.joblib', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the results\n",
    "print('out-of-bag accuracy of prediction model:')\n",
    "print(rf.oob_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now you trained your Random Forest model! And it scored with the high out-of-bag accuracy of 98%! \n",
    "\n",
    "But - is that all? Don't we want to know more? What about the explainability and deriving some knowledge out of it? Let us dive into the interpretation :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting Random Forest models with Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Permutation Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous courses you were introduced to Permutation Feature Importance. Recall, the Permutation Feature Importance is defined to be the decrease in a model score when a single feature value is randomly shuffled. This procedure breaks the relationship between the feature and the target, thus the drop in the model score is indicative of how much the model depends on the feature. Now it is time to see how it works on the penguins dataset. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = permutation_importance(rf, X_penguins, y_penguins, n_repeats=50, max_samples = 0.8, random_state=42)\n",
    "plot_permutation_feature_importance(result=result, data=X_penguins, title=\"Permutation Feature Importance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='green'>\n",
    "\n",
    "#### Question 1: How big is the influence of the most important feature on the model performance?\n",
    "\n",
    "<font color='grey'>\n",
    "\n",
    "#### Your Answer: \n",
    "\n",
    "Permutation of the feature ‘bill_length_mm’ drops the accuracy by at most 0.3 (right plot), and on average 0.25 (left plot).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Feature Importance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative for Permutation Feature Importance is the Random Forest specific Feature Importance method based on the mean decrease in impurity. The mean decrease in impurity is defined as the total decrease in node impurity averaged over all trees of the ensemble. This Feature Importances is directly provided by the fitted attribute _feature_importances__ .\n",
    "\n",
    "Lets plot the feature importance based on mean decrease in impurity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_impurity_feature_importance(rf.feature_importances_, names=X_penguins.columns, title=\"Random Forest Feature Importance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='green'>\n",
    "\n",
    "#### Question 2: Inspect the differences between the results of the two feature importance plots. What do you notice? \n",
    "_Hint:_ Take a look at the correlation plot below (run the cell to see it)\n",
    "\n",
    "<font color='grey'>\n",
    "\n",
    "#### Your Answer: \n",
    "\n",
    "1. Random Forest Feature Importance identifies more important features than the Permutation Feature Importance.\n",
    "2. It seems that the feature importance of the correlated features flipper_length and body_mass are artificially lower due to the high correlation. Random Forest Feature Importance does not seem to be affected by this correlation effect. This shows that Permutation Feature Importance results should be interpreted with great care in the presence of correlated features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_correlation_matrix(X_penguins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though the Random Forest Feature Importance does overcome some disadvantages of Permutation Feature Importance, it does not give us more information about the class-specific differences and further insights into the decision paths of the Random Forest model. Therefore, we developed a Random Forest specific interpretability method called Forest-Guided Clustering (FGC) that leverages the tree structure of Random Forest models to get insights into the decision making process of the model. Let us dive into the method... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting Random Forest models with Forest-Guided Clustering\n",
    "\n",
    "We prepared a small video lecture for you as an Introduction to Forest-Guided Clustering.\n",
    "For additional information you can have a look at the documentation for the FGC: https://forest-guided-clustering.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import VimeoVideo\n",
    "\n",
    "VimeoVideo(\"746443233?h=07ddf2290b\", width=800, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use FGC to gain more insights into the decision making process of the Random Forest model we trained previously. Afterwards, we will compare the feature importance results obtained by the previous methods and with FGC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an FGC instance\n",
    "fgc = FgClustering(model=rf, data=X_penguins, target_column=y_penguins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FGC is based on the K-Medoids clustering algorithm, which requires a predefined number of clusters as input. FGC is able to optimize the number of clusters based on a scoring function, which is minimizing the model bias while restricting the model complexity. The argument _number_of_clusters_ is used to either pass the predefined number of clusters or should be left empty if optimization is desired. \n",
    "\n",
    "For the sake of example and since the optimization part takes some time, we will set the number of cluster equal to the number of species present in the penguins dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the fgc instance:\n",
    "fgc.run(number_of_clusters=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FGC provides couple of ways to visualise the results and help interpret them:\n",
    "\n",
    "- visualise global and local feature importance: features that show different and concise value distributions across clusters are defined to be globally or locally important\n",
    "- reveal the decision rules of RF model by visualizing feature patterns per cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global and Local Feature importance provided by FGC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Global feature importance** is represented as the significance of the difference between cluster-wise feature distributions as a measure of global feature importance (ANOVA for continuous features and chi square for categorical features). **Features which have significantly different distributions across clusters have a high feature importance**, while features which have a similar feature distribution across clusters have a low feature importance.\n",
    "\n",
    "In addition to the global feature importance, we also provide a **local feature importance**, which gives the **importance of each feature for each cluster**. For the local feature importance we pre-filter the features based on the global feature importance (_thr_pvalue_ is used for the filtering step, just as in the plots before). Here, a feature is considered important if its distribution in a particlular cluster is clearly different from the feature distribution in the whole dataset.\n",
    "\n",
    "Please note that the importance is defined as 1 - _p_-value. Therefore, the more significant difference across clusters the feature shows (the smaller the _p_-value), the closer the importance of the feature is to 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Plot global feature importance\n",
    "fgc.plot_global_feature_importance()\n",
    "# Plot local feature importance\n",
    "fgc.plot_local_feature_importance(thr_pvalue=1) # Set thr_pvalue=1 to show all the features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='green'>\n",
    "\n",
    "#### Question 3: What do you observe when comparing the Random Forest Feature Importance and the FGC Feature Importance?\n",
    "\n",
    "\n",
    "<font color='grey'>\n",
    "\n",
    "#### Your Answer: \n",
    "\n",
    "The global feature importance gives us the same results as the Random Forest Feature Importance. The local feature importance reveals more information. For example, the feature island is important for the cluster 1 and 2, but not for the cluster 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the decision paths of the Random Forest model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forest-Guided Clustering provides the special option to visualize the decision path of a Random Forest model, reflecting the decision making process of that model, in a heatmap summary plot and a feature-wise distribution plot. The heatmap provides a general overview on the target value attribution and feature enrichment / depletion per cluster.  We can see which classes/target values fall into which cluster and samples that fall into the \"wrong\" cluster can be inspected further as they might be extreme outliers or wrongly labelled samples / measurement errors. The distribution plots contain the same information as the heatmap just presented in a different way. Here the features are not standardized and we can see the actual scale of each feature on the y axis. Furthermore, we get an idea of the distribution of feature values within each cluster, e.g. having a small or high within-cluster-variation. \n",
    "\n",
    "We can choose which features we want to plot by specifying the _p_-value threshold applied to the _p_-values of the features from the global feature importance calculation. The default threshold _thr_pvalue_ is set to 0.01. By selecting a lower _p_-value threshold, we only plot features that show high differences between cluster-wise feature distributions. \n",
    "\n",
    "Remember, we were transforming our target variable into integeres with the LabelEncoder instance. The resulting mapping from that process looks like this:\n",
    "\n",
    "- Adelie = 0\n",
    "- Chinstrap = 1\n",
    "- Gentoo = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fgc.plot_decision_paths(thr_pvalue=0.01) # feel free to try different p-values thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='green'>\n",
    "\n",
    "#### Question 4: Why are the features 'sex' and 'year' not shown on the plots above?\n",
    "\n",
    "\n",
    "<font color='grey'>\n",
    "\n",
    "#### Your Answer: \n",
    "\n",
    "We only show features with p-values < thr_pvalue = 0.01. This means that these two features don’t show significant difference between clusters. Hence, they don’t seem to play a role in the decision making process of this random forest model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='green'>\n",
    "\n",
    "#### Question 5: Try to describe species by observing the plots (Adelie = 0, Chinstrap = 1, Gentoo = 2). Use the following examples to guide you:\n",
    "\n",
    "- What makes Gentoo different from the other two species? \n",
    "- What makes Chinstrap different from Adelie?\n",
    "- ...\n",
    "\n",
    "<center><img src=\"./figures/bill_length.png\" width=\"200\" /></center>\n",
    "\n",
    "\n",
    "<font color='grey'>\n",
    "\n",
    "#### Your Answer: \n",
    "\n",
    "Some possible answers are:\n",
    "\n",
    "- Gento has a larger body mass and smaller bill depth\n",
    "- Adelie has smaller bill lenght\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "27d5093e17a4b198c65deae77daed9c435447949e4da0fec89e0c09b8a0f3bc9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
