{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![logo](https://github.com/donatellacea/DL_tutorials/blob/main/notebooks/figures/1128-191-max.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **XAI in Deep Learning-Based Signal Analysis: Transformers**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Notebook, we introduce the concept of transformers in machine learning, highlighting their significance in natural language processing (NLP).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "### Setup Colab environment\n",
    "\n",
    "If you installed the packages and requirements on your own machine, you can skip this section and start from the import section.\n",
    "Otherwise, you can follow and execute the tutorial on your browser. In order to start working on the notebook, click on the following button, this will open this page in the Colab environment and you will be able to execute the code on your own.\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/HelmholtzAI-Consultants-Munich/Zero2Hero---Introduction-to-XAI/blob/Juelich-2023/data_and_models/Model-Transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you opened the notebook in Colab, follow the next step:\n",
    "\n",
    "1. Run this cell to connect your Google Drive to Colab and install packages\n",
    "2. Allow this notebook to access your Google Drive files. Click on 'Yes', and select your account.\n",
    "3. \"Google Drive for desktop wants to access your Google Account\". Click on 'Allow'.\n",
    "   \n",
    "At this point, a folder has been created in your Drive and you can navigate it through the lefthand panel in Colab, you might also have received an email that informs you about the access on your Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "%cd /content/drive/MyDrive\n",
    "!git clone --branch Juelich-2023 https://github.com/HelmholtzAI-Consultants-Munich/XAI-Tutorials.git\n",
    "%cd XAI-Tutorials/data_and_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. **Encoder-Decoder Structure**\n",
    " \n",
    "Transformers are a type of neural network architecture that have become a cornerstone in the field of natural language processing (NLP) and beyond. They were introduced in the 2017 paper \"Attention is All You Need\" by Vaswani et al. Transformers are a milestone because of the **Attention Mechanism**, especially self-attention. \n",
    "\n",
    "The Transformer architecture is divided into two main parts:\n",
    "\n",
    "1. **Encoder:**\n",
    "   - The encoder processes the input data (like a sentence in a translation task) and encodes it into a context-rich representation. It consists of a stack of layers, each containing a self-attention mechanism and a feed-forward neural network.\n",
    "\n",
    "2. **Decoder:**\n",
    "   - The decoder takes the encoded input and generates the output sequence (like a translated sentence). It also consists of a stack of layers, each with two attention mechanisms (one that attends to the output of the encoder and one that is a masked self-attention mechanism to prevent the decoder from seeing future tokens in the output sequence) and a feed-forward neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"..//docs/source/_figures/transformers.png\" alt=\"transformers\" width=\"700\" height=\"700\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transformer model\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, input_vocab_size, output_vocab_size, hidden_size, num_layers, num_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(input_vocab_size, hidden_size, num_layers, num_heads, dropout)\n",
    "        self.decoder = Decoder(output_vocab_size, hidden_size, num_layers, num_heads, dropout)\n",
    "        self.output_layer = nn.Linear(hidden_size, output_vocab_size)\n",
    "\n",
    "    def forward(self, input_seq, output_seq):\n",
    "        encoder_output, encoder_mask = self.encoder(input_seq)\n",
    "        decoder_output, decoder_mask = self.decoder(output_seq, encoder_output, encoder_mask)\n",
    "        output_logits = self.output_layer(decoder_output)\n",
    "        return output_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. **Input Processing**\n",
    "\n",
    "Before feeding the input into the transformer encoder, some steps are performed: \n",
    "- **Tokenization:** is a fundamental step in text processing and NLP. It involves splitting text into smaller units, called tokens. Tokens are often words, but they can also be characters, subwords, or even sentences, depending on the level of tokenization. \n",
    "Example:\n",
    "Text: \"Natural Language Processing is fascinating.\"\n",
    "Tokens: [\"Natural\", \"Language\", \"Processing\", \"is\", \"fascinating\"]\n",
    "\n",
    "- **Input Embeddings:** The input sequence (e.g., a sentence) is converted into a sequence of vectors. This is done through embeddings which map words or tokens to high-dimensional vectors.\n",
    "\n",
    "- **Positional Encodings:** Since the Transformer does not have recurrent or convolutional layers, it uses positional encodings to add information about the position of each token in the sequence. These positional encodings have the same dimension as the embeddings and are added to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the encoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, num_layers, num_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.positional_encoding = PositionalEncoding(hidden_size, dropout)\n",
    "        self.layers = nn.ModuleList([EncoderLayer(hidden_size, num_heads, dropout) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        input_embedded = self.embedding(input_seq)\n",
    "        input_encoded = self.positional_encoding(input_embedded)\n",
    "        encoder_mask = input_seq == 0\n",
    "        for layer in self.layers:\n",
    "            input_encoded = layer(input_encoded, encoder_mask)\n",
    "        return input_encoded, encoder_mask\n",
    "    \n",
    "# Define the positional encoding layer\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, hidden_size, dropout, max_length=5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        positional_encoding = torch.zeros(max_length, hidden_size)\n",
    "        position = torch.arange(0, max_length, dtype=torch.float32).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, hidden_size, 2).float() * (-math.log(10000.0) / hidden_size))\n",
    "        positional_encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        positional_encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('positional_encoding', positional_encoding)\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        input_tensor = input_tensor + self.positional_encoding[:input_tensor.size(1), :].unsqueeze(0)\n",
    "        input_tensor = self.dropout(input_tensor)\n",
    "        return input_tensor\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. **The Encoder Part**\n",
    "\n",
    "The encoder in the Transformer architecture is designed to process and encode input sequences. It's a critical component for understanding and representing the input data in a form that the decoder can then use for tasks like translation or text generation. \n",
    "The encoder part is composed of:\n",
    "\n",
    "- **Stack of Layers:** The encoder is composed of a stack of identical layers. The number of layers varies (e.g., the original Transformer model uses 6 layers), but each layer has the same structure.\n",
    "- **Two Sub-Layers in Each Encoder Layer:**\n",
    "  - A multi-head self-attention mechanism.\n",
    "  - A position-wise fully connected feed-forward network.\n",
    "\n",
    "The output of the final encoder layer is a sequence of vectors representing the input sequence. This output is then used as the input for the Transformer decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the encoder layer\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.self_attention = MultiHeadAttention(hidden_size, num_heads, dropout)\n",
    "        self.feed_forward = FeedForward(hidden_size, dropout)\n",
    "\n",
    "    def forward(self, input_encoded, encoder_mask):\n",
    "        self_attention_output, _ = self.self_attention(input_encoded, input_encoded, input_encoded, encoder_mask)\n",
    "        feed_forward_output = self.feed_forward(self_attention_output)\n",
    "        return feed_forward_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. **The Self-Attention Mechanism**\n",
    "\n",
    "The goal of self-attention is to generate a representation of each element in a sequence by considering the entire sequence. For example, in a sentence, the representation of a word is computed by attending to all words in the sentence, including the word itself.\n",
    "\n",
    "Components:\n",
    "Queries, Keys, and Values: For each element in the input sequence, three vectors are computed: a query vector (Q), a key vector (K), and a value vector (V). These are typically created by multiplying the input embeddings with three different weight matrices.\n",
    "\n",
    "Attention Scores:\n",
    "The model calculates the attention score for each pair of elements in the sequence. This is typically done by taking the dot product of the query vector of one element with the key vector of another, which indicates how much focus to put on other parts of the input sequence when encoding a particular element.\n",
    "\n",
    "Scaling and Normalization:\n",
    "The dot product scores are scaled down (usually by the square root of the dimension of the key vectors), and a softmax function is applied to obtain the final attention weights. This normalization ensures that the weights across the sequence sum up to 1.\n",
    "\n",
    "Weighted Sum:\n",
    "The output for each element is then a weighted sum of the value vectors, where the weights are the attention scores. This results in a new representation for each element that incorporates information from the entire sequence.\n",
    "\n",
    "<img src=\"..//docs/source/_figures/self-attention.png\" alt=\"self-attention\" width=\"700\" height=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. **The Mechanism of Multi-Head Attention**\n",
    "\n",
    "In multi-head attention, the attention mechanism is run in parallel multiple times. Each parallel run is known as a \"head.\"\n",
    "Each head learns to pay attention to different parts of the input, allowing the model to capture various aspects of the information (like different types of syntactic or semantic relationships).\n",
    "\n",
    "The outputs of all attention heads are concatenated and then linearly transformed into the final output. This combination allows the model to pay attention to information from different representation subspaces at different positions.\n",
    "\n",
    "\n",
    "<img src=\"..//docs/source/_figures/multi-head-attention.png\" alt=\"multi-head-attention\" width=\"700\" height=\"700\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the multi-head attention layer\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_size = hidden_size // num_heads\n",
    "        self.query = nn.Linear(hidden_size, hidden_size)\n",
    "        self.key = nn.Linear(hidden_size, hidden_size)\n",
    "        self.value = nn.Linear(hidden_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.output_layer = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "        query = self.query(query).view(batch_size, -1, self.num_heads, self.head_size).transpose(1, 2)\n",
    "        key = self.key(key).view(batch_size, -1, self.num_heads, self.head_size).transpose(1, 2)\n",
    "        value = self.value(value).view(batch_size, -1, self.num_heads, self.head_size).transpose(1, 2)\n",
    "        attention_scores = torch.matmul(query, key.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.head_size, dtype=torch.float32))\n",
    "        if mask is not None:\n",
    "            attention_scores = attention_scores.masked_fill(mask.unsqueeze(1).unsqueeze(2) == 0, float('-inf'))\n",
    "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "        attention_output = torch.matmul(attention_probs, value)\n",
    "        attention_output = attention_output.transpose(1, 2).contiguous().view(batch_size, -1, self.hidden_size)\n",
    "        output = self.output_layer(attention_output)\n",
    "        return output, attention_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. **Position-Wise Feed-Forward Networks**\n",
    "\n",
    "- **Local Processing:** Each layer also contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between.\n",
    "- **Purpose:** While the self-attention layers help with representing the relationship between different words (or tokens) in the sequence, the feed-forward network helps to process each word individually.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, hidden_size, dropout):\n",
    "        super().__init__()\n",
    "        self.hidden_layer = nn.Linear(hidden_size, hidden_size * 4)\n",
    "        self.output_layer = nn.Linear(hidden_size * 4, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        hidden_output = nn.ReLU()(self.hidden_layer(input_tensor))\n",
    "        hidden_output = self.dropout(hidden_output)\n",
    "        output = self.output_layer(hidden_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. **The decoder part**\n",
    "\n",
    "In the Transformer model, the decoder is responsible for generating an output sequence, typically in tasks like machine translation, text generation, or summarization. During training, the decoder operates in conjunction with the encoder, but with some additional mechanisms that facilitate sequence generation. Here's how the decoder part works during training:\n",
    "\n",
    "### 1. **Overall Structure**\n",
    "\n",
    "The Transformer decoder has a structure similar to the encoder but with some key differences:\n",
    "\n",
    "- **Multiple Layers:** Like the encoder, the decoder is composed of multiple identical layers.\n",
    "- **Self-Attention and Encoder-Attention:** Each layer in the decoder includes two sub-layers of multi-head attention mechanisms (self-attention and encoder-attention) and a feed-forward neural network.\n",
    "\n",
    "### 2. **Input to the Decoder**\n",
    "\n",
    "- **Shifted Right:** The input to the decoder during training is typically the target sequence (what the model is expected to generate) shifted right by one position. This shifting ensures that the prediction for a certain position is made by only considering the known output up to that point.\n",
    "\n",
    "### 3. **Masked Self-Attention**\n",
    "\n",
    "- **Preventing Future Information Leakage:** The self-attention mechanism in the decoder is modified with masking to prevent positions from attending to subsequent positions. This masking ensures that the predictions for a particular word can only depend on the known outputs (words) that come before it, mimicking the sequential generation during inference.\n",
    "- **Sequential Dependence:** This mechanism is critical for learning the dependency of a word on its predecessors in the sequence.\n",
    "\n",
    "### 4. **Encoder-Decoder Attention**\n",
    "\n",
    "- **Interaction with Encoder Outputs:** After the masked self-attention, each decoder layer has an encoder-decoder attention mechanism. In this step, the queries come from the previous layer of the decoder, and the keys and values come from the output of the encoder. This allows the decoder to focus on relevant parts of the input sequence (from the encoder), which is essential for tasks like translation where alignment between input and output sequences is crucial.\n",
    "\n",
    "### 5. **Output Generation**\n",
    "\n",
    "- **Linear Layer and Softmax:** The output of the decoder's top layer is passed through a linear layer followed by a softmax layer. This step generates probabilities for each word in the model's vocabulary as the next output in the sequence.\n",
    "- **Training Objective:** During training, the decoder is trained to predict the next word in the target sequence given the previous words. This is typically done using a cross-entropy loss between the predicted probabilities and the actual next word in the sequence.\n",
    "\n",
    "### 6. **Teacher Forcing**\n",
    "\n",
    "- **Use of Actual Output for Next Input:** In training, the actual target sequence (shifted right) is used as input to the decoder. This technique, known as \"teacher forcing,\" helps stabilize and speed up training but can lead to issues like exposure bias during inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the decoder\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, num_layers, num_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.positional_encoding = PositionalEncoding(hidden_size, dropout)\n",
    "        self.layers = nn.ModuleList([DecoderLayer(hidden_size, num_heads, dropout) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, output_seq, encoder_output, encoder_mask):\n",
    "        output_embedded = self.embedding(output_seq)\n",
    "        output_encoded = self.positional_encoding(output_embedded)\n",
    "        decoder_mask = self.generate_square_subsequent_mask(output_seq.size(1))\n",
    "        for layer in self.layers:\n",
    "            output_encoded = layer(output_encoded, encoder_output, decoder_mask, encoder_mask)\n",
    "        return output_encoded\n",
    "\n",
    "    def generate_square_subsequent_mask(self, size):\n",
    "        mask = (torch.triu(torch.ones(size, size)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the decoder layer\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.self_attention = MultiHeadAttention(hidden_size, num_heads, dropout)\n",
    "        self.encoder_attention = MultiHeadAttention(hidden_size, num_heads, dropout)\n",
    "        self.feed_forward = FeedForward(hidden_size, dropout)\n",
    "\n",
    "    def forward(self, output_encoded, encoder_output, decoder_mask, encoder_mask):\n",
    "        self_attention_output, _ = self.self_attention(output_encoded, output_encoded, output_encoded, decoder_mask)\n",
    "        encoder_attention_output, _ = self.encoder_attention(self_attention_output, encoder_output, encoder_output, encoder_mask)\n",
    "        feed_forward_output = self.feed_forward(encoder_attention_output)\n",
    "        return feed_forward_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. **Put It All Together**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(5, 512)\n",
       "    (positional_encoding): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0): EncoderLayer(\n",
       "        (self_attention): MultiHeadAttention(\n",
       "          (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (output_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (hidden_layer): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (output_layer): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): EncoderLayer(\n",
       "        (self_attention): MultiHeadAttention(\n",
       "          (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (output_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (hidden_layer): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (output_layer): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): EncoderLayer(\n",
       "        (self_attention): MultiHeadAttention(\n",
       "          (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (output_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (hidden_layer): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (output_layer): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): EncoderLayer(\n",
       "        (self_attention): MultiHeadAttention(\n",
       "          (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (output_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (hidden_layer): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (output_layer): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): EncoderLayer(\n",
       "        (self_attention): MultiHeadAttention(\n",
       "          (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (output_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (hidden_layer): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (output_layer): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): EncoderLayer(\n",
       "        (self_attention): MultiHeadAttention(\n",
       "          (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (output_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (hidden_layer): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (output_layer): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(5, 512)\n",
       "    (positional_encoding): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0): DecoderLayer(\n",
       "        (self_attention): MultiHeadAttention(\n",
       "          (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (output_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (encoder_attention): MultiHeadAttention(\n",
       "          (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (output_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (hidden_layer): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (output_layer): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): DecoderLayer(\n",
       "        (self_attention): MultiHeadAttention(\n",
       "          (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (output_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (encoder_attention): MultiHeadAttention(\n",
       "          (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (output_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (hidden_layer): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (output_layer): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): DecoderLayer(\n",
       "        (self_attention): MultiHeadAttention(\n",
       "          (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (output_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (encoder_attention): MultiHeadAttention(\n",
       "          (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (output_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (hidden_layer): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (output_layer): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): DecoderLayer(\n",
       "        (self_attention): MultiHeadAttention(\n",
       "          (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (output_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (encoder_attention): MultiHeadAttention(\n",
       "          (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (output_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (hidden_layer): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (output_layer): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): DecoderLayer(\n",
       "        (self_attention): MultiHeadAttention(\n",
       "          (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (output_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (encoder_attention): MultiHeadAttention(\n",
       "          (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (output_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (hidden_layer): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (output_layer): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): DecoderLayer(\n",
       "        (self_attention): MultiHeadAttention(\n",
       "          (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (output_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (encoder_attention): MultiHeadAttention(\n",
       "          (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (output_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (hidden_layer): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (output_layer): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (output_layer): Linear(in_features=512, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Transformer(5, 5, hidden_size=512, num_layers=6, num_heads=8, dropout=0.1)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
