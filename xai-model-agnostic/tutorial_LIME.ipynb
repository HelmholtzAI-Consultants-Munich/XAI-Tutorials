{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe73dc5d",
   "metadata": {},
   "source": [
    "![logo](https://github.com/donatellacea/DL_tutorials/blob/main/notebooks/figures/1128-191-max.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9d6c15",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# Model-Agnostic Interpretation with LIME\n",
    "\n",
    "In this Notebook we will demonstrate how to use the Local Interpretable Model-Agnostic Explanations (LIME) and interpret its results.\n",
    "\n",
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9985159",
   "metadata": {},
   "source": [
    "### Setup Colab environment\n",
    "\n",
    "If you installed the packages and requirments on your own machine, you can skip this section and start from the import section.\n",
    "Otherwise you can follow and execute the tutorial on your browser. In order to start working on the notebook, click on the following button, this will open this page in the Colab environment and you will be able to execute the code on your own.\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/HelmholtzAI-Consultants-Munich/Zero2Hero---Introduction-to-XAI/blob/master/xai-model-agnostic/tutorial_LIME.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf284707",
   "metadata": {},
   "source": [
    "Now that you are visualizing the notebook in Colab, run the next cell to install the packages we will use.\n",
    "There are few things you should follow in order to properly set the notebook up:\n",
    "\n",
    "1. Warning: This notebook was not authored by Google. *Click* on 'Run anyway'.\n",
    "2. When the installation commands are done, there might be \"Restart runtime\" button at the end of the output. Please, *click* it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f6e7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e36efe4",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea45ebd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T11:51:08.501249Z",
     "iopub.status.busy": "2022-09-14T11:51:08.500786Z",
     "iopub.status.idle": "2022-09-14T11:51:10.498597Z",
     "shell.execute_reply": "2022-09-14T11:51:10.497866Z"
    },
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from lime.lime_tabular import LimeTabularExplainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f34a31",
   "metadata": {
    "cell_marker": "'''"
   },
   "source": [
    "Now, we fix the random seeds to ensure reproducible results, as we work with (pseudo) random numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f19032",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T11:51:10.503453Z",
     "iopub.status.busy": "2022-09-14T11:51:10.502768Z",
     "iopub.status.idle": "2022-09-14T11:51:10.506221Z",
     "shell.execute_reply": "2022-09-14T11:51:10.505737Z"
    }
   },
   "outputs": [],
   "source": [
    "# assert reproducible random number generation\n",
    "seed = 1\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff80af92",
   "metadata": {},
   "source": [
    "## The California Housing Dataset: Data Loading and Model Training\n",
    "\n",
    "Let's use the California housing data set. The dataset is introduced in detail in the permutation importance notebook. Check it out, if you like to have more information on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3530e6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "calif_house_data = fetch_california_housing()\n",
    "X = pd.DataFrame(calif_house_data['data'], columns = calif_house_data['feature_names'])\n",
    "y = pd.DataFrame(calif_house_data['target'], columns=['Price'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03936320",
   "metadata": {
    "cell_marker": "'''",
    "lines_to_next_cell": 2
   },
   "source": [
    "For the sake of runtime we limit ourselves to only the first 2000 samples. We will split parts of the data, so the model can not use all the available information for training. That way, we can also check performance and interpretation results on previously unseen data, mirroring the most probable practical use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b21508c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take only first 2000 samples and convert all data frames to numpy array, which is required for LIME\n",
    "feature_names = X.columns\n",
    "X = X[:2000].to_numpy()\n",
    "y = y[:2000].to_numpy()\n",
    "\n",
    "# split off the test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.75, random_state=seed)\n",
    "print('Number of training samples: {}'.format(X_train.shape[0]))\n",
    "print('Number of test samples: {}'.format(X_test.shape[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480d6509",
   "metadata": {},
   "source": [
    "We'll create a Kernel ridge regression model with an RBF kernel to predict our *price* target variable from the other features, after they have been properly rescaled to each have zero mean and unit standard deviation.  \n",
    "Don't worry for now if you are not familiar with the model. It is just meant as a protoype of a model that is not straightforward to interpret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4172d64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    (\"preprocessing\", StandardScaler()),\n",
    "    (\"model\", KernelRidge(kernel=\"rbf\"))\n",
    "])\n",
    "\n",
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a28159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# is the model performing reasonably on the training data?\n",
    "print('Model Performance on training data: {}'.format(pipe.score(X_train, y_train)))\n",
    "\n",
    "# is the model performing reasonably on the test data?\n",
    "print('Model Performance on test data: {}'.format(pipe.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b17e340",
   "metadata": {},
   "source": [
    "Since model training and tuning is not a focus of this course, we will not try to improve the model performance further.  \n",
    "**Note:** you should keep in mind that interpreting a low performing model can lead to wrong conclusions.\n",
    "\n",
    "Now that we trained a regression model which predicts the prices relatively well, but might be a bit hard to interpet directly, we can make use of LIME to help us out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7711a36",
   "metadata": {},
   "source": [
    "## Now, what does my model actually think is important in the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8630cf6",
   "metadata": {},
   "source": [
    "### Local Interpretable Model-Agnostic Explanations (LIME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7517608e",
   "metadata": {},
   "source": [
    "We will try to get insights into which features are important by carrying out a method called **LIME**.\n",
    "\n",
    "We prepared a small [video lecture](https://vimeo.com/745319036/a86f126018) for you to help you understand how LIME works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27cf20d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T11:51:08.484927Z",
     "iopub.status.busy": "2022-09-14T11:51:08.484433Z",
     "iopub.status.idle": "2022-09-14T11:51:08.497955Z",
     "shell.execute_reply": "2022-09-14T11:51:08.497475Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import VimeoVideo\n",
    "\n",
    "VimeoVideo(\"745319036?h=a86f126018\", width=800, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5803d7e",
   "metadata": {
    "cell_marker": "'''"
   },
   "source": [
    "To summarize, LIME, an abbreviation for \"local interpretable model agnostic explanations\" is an approach that tries to deliver explanations for individual samples. It works by constructing an interpretable surrogate model (like a linear regression) to approximate predictions of a more complex model in the neighborhood of a given sample.\n",
    "\n",
    "**Note:** this method is a **local** method which means that it does only provide explanations for individual samples, but not for a full dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15d5d85",
   "metadata": {},
   "source": [
    "Now lets use Permutation Feature Importance to get some insights into the Kernel Ridge regression model we trained above.  \n",
    "We first have to specify an important parameter for LIME: the *kernel_width*, which in principle determines how large the neighborhood around our sample will be. The optimal choice of this parameter is difficult and currently still an open research question and one of the main disadvantages of the method. Feel free to play around with different values and observe how the generated explanations can change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c051fef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T11:51:11.425513Z",
     "iopub.status.busy": "2022-09-14T11:51:11.424758Z",
     "iopub.status.idle": "2022-09-14T11:51:11.427849Z",
     "shell.execute_reply": "2022-09-14T11:51:11.427355Z"
    }
   },
   "outputs": [],
   "source": [
    "kernel_width = 0.5 # default is 0.75 * sqrt(n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4726b6de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T11:51:11.430750Z",
     "iopub.status.busy": "2022-09-14T11:51:11.430020Z",
     "iopub.status.idle": "2022-09-14T11:51:11.434438Z",
     "shell.execute_reply": "2022-09-14T11:51:11.433969Z"
    }
   },
   "outputs": [],
   "source": [
    "explainer = LimeTabularExplainer(\n",
    "    training_data=X_train,\n",
    "    mode=\"regression\",\n",
    "    training_labels=y_train,\n",
    "    feature_names=feature_names,\n",
    "    feature_selection='none',  # before applying the surrogate model, one could also select features\n",
    "    random_state=seed,\n",
    "    sample_around_instance=True,  # NOTE: default is False!\n",
    "    kernel_width=kernel_width,  \n",
    "    discretize_continuous=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501994c1",
   "metadata": {},
   "source": [
    "Once we have defined the setup for LIME, we have to choose an instance for which we want to compute the local surrogate model to get explanations for the instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20845a95",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T11:51:11.437363Z",
     "iopub.status.busy": "2022-09-14T11:51:11.436634Z",
     "iopub.status.idle": "2022-09-14T11:51:11.561432Z",
     "shell.execute_reply": "2022-09-14T11:51:11.560818Z"
    }
   },
   "outputs": [],
   "source": [
    "# choose an instance that you want to explain\n",
    "inst_idx = 0\n",
    "\n",
    "print(f\"Instance {inst_idx} of training data will be explained.\")\n",
    "\n",
    "# number of samples in the neighborhood of our point that we will use to fit\n",
    "# a simpler surrogate model to explain the original complex models predictions\n",
    "num_samples = 5000\n",
    "\n",
    "instance = X_train[inst_idx]\n",
    "instance_label = y_train[inst_idx]\n",
    "\n",
    "explanation = explainer.explain_instance(\n",
    "    data_row=instance,\n",
    "    predict_fn=pipe.predict,\n",
    "    labels=instance_label,\n",
    "    model_regressor=LinearRegression(),\n",
    "    num_samples=num_samples, # size of the neighborhood\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288beca2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T11:51:11.565309Z",
     "iopub.status.busy": "2022-09-14T11:51:11.564166Z",
     "iopub.status.idle": "2022-09-14T11:51:11.930105Z",
     "shell.execute_reply": "2022-09-14T11:51:11.929470Z"
    }
   },
   "outputs": [],
   "source": [
    "explanation.as_pyplot_figure();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42af93f",
   "metadata": {},
   "source": [
    "All that LIME did was to fit a linear regression model to approximate the complex model's predictions. \n",
    "The dataset for creating the fit were the neighborhood samples that were randomly created around our selected instance.  \n",
    "Linear regression models estimate a single parameter for each feature and those are plotted above. \n",
    "They indicate how much each feature contributes to the approximation of the complex model's predictions.\n",
    "Two of the most informative features for the model are the *median income* (MedInc) and the *average number of household members* (AveOccup), which are the features with the highest absolute model coefficients. \n",
    "The positive sign of the *median income* coefficient indicates a positive relation between this feature and the target variable (*price*), i.e. a higher income leads to higher price predictions. \n",
    "On the other hand, the negative sign of the *average number of household members* coefficient suggests that as the average number of household members increases, the price tends to decrease.  \n",
    "\n",
    "The visualization below gives us a bit of additional information than the barplots themselves:\n",
    "\n",
    "- For the predicted value, min and max are the range of predictions of our complicated model on the neighboorhood samples\n",
    "while the value itself is the prediction of the complex model at the instance we want to interpret.\n",
    "- The barplot is the same as above, showing model coefficients of our simple surrogate model.\n",
    "- The table on the right summarizes the feature representation of the instance we wanted to interpret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194bad9b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T11:51:11.933079Z",
     "iopub.status.busy": "2022-09-14T11:51:11.932654Z",
     "iopub.status.idle": "2022-09-14T11:51:11.956131Z",
     "shell.execute_reply": "2022-09-14T11:51:11.955485Z"
    }
   },
   "outputs": [],
   "source": [
    "explanation.show_in_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5353056e",
   "metadata": {},
   "source": [
    "To measure how well the simpler surrogate model is able to approximate the predictions of the more complex model, we can use any metric that summarizes the quality of the predictions that the surrogate model makes. \n",
    "This chosen metric can serve as \"fidelity measure\", which indicates how reliable the interpretable model is. \n",
    "Even though the choice of the fidelity measure is up to you, it is important to assess the predictive ability of the surrogate model when LIME explanations are to be used!\n",
    "How much would you trust explanations delivered from a surrogate model that can not reasonably approximate the complex models predictions?\n",
    "\n",
    "In our case, we used a linear regression model as surrogate model. A commonly used metric to quantify the goodness of fit for the linear regression model is the $R^2$ score. \n",
    "It shows how well the linear model is able to approximate the predictions of the more complex model on the neighborhood samples. $R^2$ scores closer to 1 indicate better approximations.\n",
    "Our surrogate model archieves an $R^2$ score of > 0.87, indicating that the explanations given by that model can be trusted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fc5905",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T11:51:11.980639Z",
     "iopub.status.busy": "2022-09-14T11:51:11.980418Z",
     "iopub.status.idle": "2022-09-14T11:51:11.986000Z",
     "shell.execute_reply": "2022-09-14T11:51:11.985195Z"
    }
   },
   "outputs": [],
   "source": [
    "explanation.score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc3fa79",
   "metadata": {},
   "source": [
    "<font color='green'>\n",
    "\n",
    "#### Question 1: What is a surrogate model?\n",
    "\n",
    "<font color='grey'>\n",
    "\n",
    "#### Your Answer: \n",
    "\n",
    "\n",
    "\n",
    "<font color='green'>\n",
    "\n",
    "#### Question 2: How is LIME using surrogate models to explain a model prediction?\n",
    "\n",
    "<font color='grey'>\n",
    "\n",
    "#### Your Answer: \n",
    "\n",
    "\n",
    "\n",
    "<font color='green'>\n",
    "\n",
    "#### Question 3: What are the main difference to the other two methods?\n",
    "\n",
    "<font color='grey'>\n",
    "\n",
    "#### Your Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311d26e9",
   "metadata": {},
   "source": [
    "## Extra Material: LIME computation step by step\n",
    "\n",
    "To get a better understanding of LIME, we will now guide you step by step through the algorthm. Even though LIME offers an easy to use API, it can be beneficial to have a quick look behind the scenes to fully understand what is going on. LIME is especially suitable here since the basic algorithm can be programmed in just a few steps.\n",
    "\n",
    "[Cristian Arteaga](https://nbviewer.org/urls/arteagac.github.io/blog/lime.ipynb) has also prepared a nice step-by-step explanation of LIME for a 2D toy problem and we recommend to take a look at his notebook which contains nice visualizations.\n",
    "Here, we are focussing on tabular data, but Christian also provides a notebook that demonstrates how LIME can work on other modalities like images as well, which is one of its big strengths!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10ef928",
   "metadata": {},
   "source": [
    "### Step 1\n",
    "First, we select an instance for which we want to explain the prediction.\n",
    "We generate many normally distributed random samples that will serve as our sample neighborhood.\n",
    "The samples expected value coincides with our instance to ensure sufficient similarity between the neighborhood and our instance, while the standard deviation is estimated from the training data.\n",
    "\n",
    "Also note that we make our instance itself part of the neighborhood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25442bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T11:51:11.991398Z",
     "iopub.status.busy": "2022-09-14T11:51:11.991192Z",
     "iopub.status.idle": "2022-09-14T11:51:11.997876Z",
     "shell.execute_reply": "2022-09-14T11:51:11.997093Z"
    },
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "\n",
    "x = instance\n",
    "\n",
    "# 1) generate random perturbations around our selected instance\n",
    "# with given mean and standard deviation\n",
    "std = X_train.std(axis=0)\n",
    "\n",
    "# NOTE: there are two options on setting the mean of the samples.\n",
    "# The default in LIME is to set it to the mean value of the training data.\n",
    "# However, it may be a better idea to set the mean to the instance itself\n",
    "# (in LIME this is done by sample_around_instance=True) in order\n",
    "# to generate samples similar to our instance with high probability.\n",
    "\n",
    "#mu = X_train.mean(axis=0)\n",
    "mu = x\n",
    "\n",
    "neighbors_of_x = np.random.normal(mu, std, size=(num_samples, X_train.shape[1]))\n",
    "\n",
    "# sneek in the instance itself as part of the neighbors\n",
    "neighbors_of_x[0] = x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cef6ff",
   "metadata": {},
   "source": [
    "### Step 2\n",
    "We let our original model predict the outcomes of the neighborhood samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7b3931",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T11:51:12.000704Z",
     "iopub.status.busy": "2022-09-14T11:51:12.000503Z",
     "iopub.status.idle": "2022-09-14T11:51:12.103413Z",
     "shell.execute_reply": "2022-09-14T11:51:12.102383Z"
    }
   },
   "outputs": [],
   "source": [
    "neighbors_of_x_pipe_pred = pipe.predict(neighbors_of_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653d1223",
   "metadata": {},
   "source": [
    "### Step 3\n",
    "We compute the distance of each neighbor to our instance and transform it to a weight which we will later use to fit our local surrogate model.\n",
    "Note that distances are computed on standardized data in order to avoid numerical instabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7ded54",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T11:51:12.107959Z",
     "iopub.status.busy": "2022-09-14T11:51:12.107336Z",
     "iopub.status.idle": "2022-09-14T11:51:12.114074Z",
     "shell.execute_reply": "2022-09-14T11:51:12.113321Z"
    }
   },
   "outputs": [],
   "source": [
    "# 3) compute euclidean distance of each neighbor to x but scale the data before using the mean and standard deviation of the\n",
    "#    training data\n",
    "#    NOTE: distances are computed based on standardized data\n",
    "#    and neighbors[0] is set to be the standardized instance itself\n",
    "\n",
    "neighbors_of_x_scaled = pipe[\"preprocessing\"].transform(neighbors_of_x)\n",
    "x_scaled = neighbors_of_x_scaled[0]\n",
    "\n",
    "distance_to_x = np.sum((neighbors_of_x_scaled - x_scaled)**2, axis=1)\n",
    "weights = np.sqrt(np.exp(-1. * (distance_to_x / kernel_width)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e577370f",
   "metadata": {},
   "source": [
    "### Step 4\n",
    "A surrogate model is fit to approximate the complex models prediction on the neighborhood samples. We will use a linear regression model since that offers fairly straightforward explanations by looking at the estimated model coefficients.\n",
    "The fit will be performed again on the standardized samples! **Note:** LIME is not restricted to linear regression models and other easy-to-interpret surrogate models could be used like decision trees.\n",
    "\n",
    "The weights computed above will serve to indicate their importance to the fit. Models with large weights are closer to our instance and should get predicted more accurately than neighbors further apart from our instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14eb5bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T11:51:12.116751Z",
     "iopub.status.busy": "2022-09-14T11:51:12.116562Z",
     "iopub.status.idle": "2022-09-14T11:51:12.128126Z",
     "shell.execute_reply": "2022-09-14T11:51:12.127629Z"
    }
   },
   "outputs": [],
   "source": [
    "# fit an explainable model on the scaled data to approximate the predictions of the complex model\n",
    "#    on the neighborhood samples\n",
    "explainable_model = LinearRegression()\n",
    "explainable_model.fit(\n",
    "    neighbors_of_x_scaled, \n",
    "    neighbors_of_x_pipe_pred,\n",
    "    sample_weight=weights)\n",
    "\n",
    "score = explainable_model.score(neighbors_of_x_scaled, neighbors_of_x_pipe_pred, sample_weight=weights)\n",
    "print(\"\\nModel performance\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7436f2",
   "metadata": {},
   "source": [
    "### Step 5\n",
    "We visualize the model coefficients to obtain a similar explanation as the LIME API offers us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe1a034",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-14T11:51:12.131174Z",
     "iopub.status.busy": "2022-09-14T11:51:12.130398Z",
     "iopub.status.idle": "2022-09-14T11:51:12.330097Z",
     "shell.execute_reply": "2022-09-14T11:51:12.329486Z"
    }
   },
   "outputs": [],
   "source": [
    "# get model coefficients\n",
    "coef_dict = dict(zip(feature_names, explainable_model.coef_[0]))\n",
    "coef_dict_sorted = dict(sorted(coef_dict.items(), key=lambda x: np.abs(x[1]), reverse=False))\n",
    "coef_df = pd.DataFrame.from_dict(coef_dict_sorted, columns=['model coefficient'], orient=\"index\")\n",
    "\n",
    "# plot similar as LIME provides\n",
    "f, ax = plt.subplots(1, 1, figsize=(9, 7))\n",
    "coef_df.plot(kind='barh', ax=ax)\n",
    "plt.title('LIME - local model coefficients')\n",
    "plt.xlabel('model coefficient')\n",
    "plt.ylabel('feature')\n",
    "plt.axvline(x=0, color='.5')\n",
    "plt.subplots_adjust(left=.3)\n",
    "_ = ax.set_yticklabels(coef_df.index)\n",
    "ax.get_legend().remove()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
