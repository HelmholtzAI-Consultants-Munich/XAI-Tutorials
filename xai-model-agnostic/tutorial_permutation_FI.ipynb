{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84308568",
   "metadata": {},
   "source": [
    "![logo](https://github.com/donatellacea/DL_tutorials/blob/main/notebooks/figures/1128-191-max.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55296300",
   "metadata": {},
   "source": [
    "# Model-Agnostic Interpretation with Permutation Feature Importance\n",
    "\n",
    "In this Notebook we will demonstrate how to use the Permutation Feature Importance method and interpret its results.\n",
    "\n",
    "--------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44db4dc4",
   "metadata": {
    "colab_type": "text",
    "id": "XYNno0xtOFJ-"
   },
   "source": [
    "### Setup Colab environment\n",
    "\n",
    "If you installed the packages and requirments on your own machine, you can skip this section and start from the import section.\n",
    "Otherwise you can follow and execute the tutorial on your browser. In order to start working on the notebook, click on the following button, this will open this page in the Colab environment and you will be able to execute the code on your own.\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/HelmholtzAI-Consultants-Munich/Zero2Hero---Introduction-to-XAI/blob/master/xai-model-agnostic/tutorial_permutation_FI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358949e2",
   "metadata": {},
   "source": [
    "After the first execution you might receive some warning and notification, please follow these instructions:  \n",
    "\n",
    "1. Warning: This notebook was not authored by Google.*  Click on 'Run anyway'.\n",
    "2. Permit this notebook to access your Google Drive files? Click on 'Yes', and select your account.\n",
    "3. Google Drive for desktop wants to access your Google Account. Click on 'Allow'.\n",
    "\n",
    "At this point, a folder has been created and you can navigate it through the lefthand panel in Colab, you might also have received an email that informs you about the access on your Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467b407b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a folder in your Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab17363d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd drive/MyDrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c2462a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't run this cell if you already cloned the repo\n",
    "!git clone https://github.com/HelmholtzAI-Consultants-Munich/Zero2Hero---Introduction-to-XAI.git  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d5e3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd Zero2Hero---Introduction-to-XAI/xai-model-agnostic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e36efe4",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a005356f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from utils.utils import * \n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import get_scorer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.inspection import permutation_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36832b46",
   "metadata": {
    "cell_marker": "'''"
   },
   "source": [
    "We fix the random seeds to ensure reproducible results, as we work with (pseudo) random numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83517e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert reproducible random number generation\n",
    "seed = 1\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff80af92",
   "metadata": {},
   "source": [
    "## The California Housing Dataset: Data Loading and Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe9a0bd",
   "metadata": {
    "cell_marker": "'''",
    "lines_to_next_cell": 0
   },
   "source": [
    "Let's use the California housing data set. The data pertains to the house prices found in a given California district and some summary statistics about them based on the 1990 census data.\n",
    "\n",
    "<center><img src=\"./figures/california_housing.jpg\" width=\"1200\" /></center>\n",
    "\n",
    "<font size=1> Source:\\\n",
    "https://www.kaggle.com/datasets/harrywang/housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173484c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "calif_house_data = fetch_california_housing()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421cb062",
   "metadata": {},
   "source": [
    "Now, let's have a look at the description of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9e8ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(calif_house_data['DESCR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4a7302",
   "metadata": {},
   "source": [
    "Let's put all of our data into a data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cd0598",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "X = pd.DataFrame(calif_house_data['data'], columns = calif_house_data['feature_names'])\n",
    "y = pd.DataFrame(calif_house_data['target'], columns=['Price'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29caebef",
   "metadata": {},
   "source": [
    "See how many samples and features we have in the dataset and how their values are distributed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e03c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Features: {}'.format(list(X.columns)))\n",
    "print('# samples: {}, # features: {}'.format(len(X.index), len(X.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba0a4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d494f886",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aebcd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd61c06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb937d1",
   "metadata": {
    "cell_marker": "'''"
   },
   "source": [
    "The data set has 20640 observations, 8 features, and 1 target variable. **Note:** each row in the dataset represents a block of houses, not a single household.  \n",
    "The target variable *price* is continuous and given in 100.000 $, so we can predict it using the other available features with a regression model of our choice.\n",
    "\n",
    "For the sake of runtime we limit ourselves to only the first 2000 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91e35cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[:2000]\n",
    "y = y[:2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6c4387",
   "metadata": {
    "cell_marker": "'''"
   },
   "source": [
    "We will split parts of the data, so the model can not use all the available information for training.  \n",
    "That way, we can also check performance and interpretation results on previously unseen data, mirroring the most probable practical use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9df4fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split off the test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.75, random_state=seed)\n",
    "print('Number of training samples: {}'.format(len(X_train.index)))\n",
    "print('Number of training samples: {}'.format(len(X_test.index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cbab6b",
   "metadata": {},
   "source": [
    "We'll create a Kernel ridge regression model with an RBF kernel to predict our *price* target variable from the other features, after they have been properly rescaled to each have zero mean and unit standard deviation.  \n",
    "Don't worry for now if you are not familiar with the model. It is just meant as a protoype of a model that is not straightforward to interpret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4172d64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    (\"preprocessing\", StandardScaler()),\n",
    "    (\"model\", KernelRidge(kernel=\"rbf\"))\n",
    "])\n",
    "\n",
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a2c892",
   "metadata": {
    "cell_marker": "'''",
    "lines_to_next_cell": 0
   },
   "source": [
    "Now that the model is fit, let's check its performance as measured by the $R^2$ metric for both the training and the validation data. $R^2$ is the coefficient of determination and the closer this value is to 1, the better our model explains the data. A constant model that always predicts the average target value disregarding the input features would get an $R^2$ score of 0.  \n",
    "**Note:** $R^2$ score can also be negative because the model can be arbitrarily worse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a28159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# is the model performing reasonably on the training data?\n",
    "print('Model Performance on training data: {}'.format(pipe.score(X_train, y_train)))\n",
    "\n",
    "# is the model performing reasonably on the test data?\n",
    "print('Model Performance on test data: {}'.format(pipe.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a596d4e",
   "metadata": {
    "cell_marker": "'''",
    "lines_to_next_cell": 2
   },
   "source": [
    "Since model training and tuning is not a focus of this course, we will not try to improve the model performance further.  \n",
    "**Note:** you should keep in mind that interpreting a low performing model can lead to wrong conclusions.\n",
    "\n",
    "Now that we trained a regression model which predicts the prices relatively well but might be a bit hard to interpret directly, we can make use of permutation feature importance to help us out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496b408c",
   "metadata": {},
   "source": [
    "## Now, what does my model actually think is important in the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2416db56",
   "metadata": {},
   "source": [
    "### Permutation Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c8cccc",
   "metadata": {
    "cell_marker": "'''"
   },
   "source": [
    "We will try to get insights into which features are important by carrying out a method called **Permutation feature importance**. The following explanation of the approach is taken directly from the [scikit_learn documentation](https://scikit-learn.org/stable/modules/permutation_importance.html#permutation-importance):\n",
    "\n",
    "\"*Permutation feature importance is a model inspection technique that can be used for any fitted estimator when the data is tabular. This is especially useful for non-linear or opaque estimators. The permutation feature importance is defined to be the decrease in a model score when a single feature value is randomly shuffled. This procedure breaks the relationship between the feature and the target, thus the drop in the model score is indicative of how much the model depends on the feature. This technique benefits from being model agnostic and can be calculated many times with different permutations of the feature*\"\n",
    "\n",
    "We prepared a small [video lecture](https://vimeo.com/745319412/1e5bd15ff7) for you to help you understand how Permutation Feature Importance works. Execute the next cell to watch it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734c6ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import VimeoVideo\n",
    "\n",
    "VimeoVideo(\"745319412?h=1e5bd15ff7\", width=800, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f61d3f",
   "metadata": {
    "cell_marker": "'''"
   },
   "source": [
    "To summarize, Permutation Feature Importance for a dataset of your choice works in the following way:\n",
    "\n",
    "First, an already trained model predicts outputs and computes a performance measure for our dataset. This will serve as baseline performance.\n",
    "\n",
    "Then, we carry out the following steps for each feature (potentially repeated multiple times):\n",
    "\n",
    "1) We permute the features value across the dataset, essentially messing up any connection between the label and this feature.  \n",
    "2) We predict outputs and compute the performance measure based on our dataset with the permuted feature.  \n",
    "3) We compare the performance measure with the baseline performance.  \n",
    "4) We use the difference between baseline performance and permuted performance as an indicator of the importance of the feature.  \n",
    "\n",
    "If both performances are similar, the messed up feature did not lead to a decrease in model performance, indicating that the model did not rely heavily on the feature, hence assigning low importance. \n",
    "On the other hand, if the performance of the data with the permuted feature is much worse than the baseline performance, this shows that the model highly depended on the feature to produce good scores.\n",
    "\n",
    "**Note:** this method is a **global** method which means that it only provides explanations for a full dataset, but not for individual samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f78b11",
   "metadata": {},
   "source": [
    "Now lets use the scikit-learn implementation called `permutation_importance` to get some insights into the Kernel Ridge regression model we trained above. For measuring the performance drop when permuting a feature, we use the standard metric of our trained model, which is the $R^2$ score. Using the same score enables us to evaluate the performace drop in relation to the baseline performance.\n",
    "20 repetitions of permutation are done for each feature to get more reliable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30838b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = get_scorer(\"r2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c2bc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation_train = permutation_importance(\n",
    "    pipe,\n",
    "    X=X_train,\n",
    "    y=y_train,\n",
    "    n_repeats=20,\n",
    "    random_state=seed,\n",
    "    scoring=scorer\n",
    ")\n",
    "\n",
    "explanation_train[\"feature\"] = X_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a3c0ae",
   "metadata": {},
   "source": [
    "We will now plot bar histograms for visualizing the importance of each feature as obtained by the mean over the 20 repetitions. To judge variability, we plot the box plots of the feature importances as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a41602b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_permutation_feature_importance_with_variance(explanation_train, X_train, 'Permutation Feature Importance on Training Data for California Housing Model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5b6d1d",
   "metadata": {},
   "source": [
    "Features with the largest importance score are the ones whose score is most decreased when the feature is permuted, indicating high relevance for prediction of the target. For our California Housing model, the *median income in a block group* seems to have the highest impact on the model performance. Permuting this feature leads to a $R^2$ score decrease of > 0.8. When comparing it to the baseline $R^2$ score of 0.8, we notice that permuting the *median income* feature leads to a model with $R^2$ score < 0, i.e. a model that is worse than a constant model that always predicts the average target value disregarding the input features.\n",
    "\n",
    "We are not restricted to obtain feature importances of the same data set that we used to train the model. Instead we could use the same approach to identify the most important features in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7b4169",
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation_test = permutation_importance(\n",
    "    pipe,\n",
    "    X=X_test,\n",
    "    y=y_test,\n",
    "    n_repeats=20,\n",
    "    random_state=seed,\n",
    "    scoring=scorer)\n",
    "\n",
    "explanation_test[\"feature\"] = X_test.columns\n",
    "\n",
    "plot_permutation_feature_importance_with_variance(explanation_test, X_test, 'Permutation Feature Importance on Test Data for California Housing Model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48da5fc1",
   "metadata": {
    "cell_marker": "'''"
   },
   "source": [
    "It seems that for both datasets, largely the same features are identified as important, which is reassuring.\n",
    "**Note:** this agreement of important features between training and testing datasets is not guaranteed. In such cases, it is not straightforward to decide on the \"truly important\" features for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a7db12",
   "metadata": {},
   "source": [
    "### Dealing with correlated features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdb13b6",
   "metadata": {
    "cell_marker": "'''"
   },
   "source": [
    "In the video you already learned that applying permutation importance to datasets that contain correlated features can be problematic in various ways.\n",
    "This is also emphasized in the [scikit_learn documentation](https://scikit-learn.org/stable/modules/permutation_importance.html#permutation-importance):\n",
    "\n",
    "\"*When two features are correlated and one of the features is permuted, the model will still have access to the feature through its correlated feature. This will result in a lower importance value for both features, where they might actually be important.*\"\n",
    "\n",
    "But this is not the only effect that can occur. Below we will see an example where importance is attributed to a feature which is completely unrelated to the target variable because of its correlation to an actually important feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d6d3eb",
   "metadata": {
    "cell_marker": "'''"
   },
   "source": [
    "Let's create toy data of multiple features of which the first two are highly correlated but only one\n",
    "of them will be used for computing our target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fad98a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create toy features\n",
    "np.random.seed(seed)\n",
    "\n",
    "corr1 = 0.95\n",
    "n_vars = 5\n",
    "\n",
    "means = np.array([i*5 for i in range(n_vars)])\n",
    "cov = np.diag(np.ones(n_vars))\n",
    "cov[0, 1] = corr1\n",
    "cov[1, 0] = corr1\n",
    "\n",
    "features = np.random.multivariate_normal(means, cov, size=2000)\n",
    "features = pd.DataFrame(features)\n",
    "features.columns = ['feature_1','feature_2','feature_3','feature_4','feature_5']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f11cdc",
   "metadata": {},
   "source": [
    "Now lets create a target variable that is computed from all features BUT feature 1, which is highly correlated to feature 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2025eac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute target from four features and add noise\n",
    "np.random.seed(seed)\n",
    "target_clean = 1.e-1 * np.exp(features.iloc[:, 1]) * 0.3 * np.sin(features.iloc[:, 2]) + 0.5 * np.sin(features.iloc[:, 3]) + 0.1 * np.cos(features.iloc[:, 4])\n",
    "target = target_clean + .02 * np.random.normal(0, 1, size=len(target_clean))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74933d2",
   "metadata": {},
   "source": [
    "As we can see in the plot below, feature 1 and feature 2 are highly correlated, while the remaining feature show no correlation amongst each other.  \n",
    "In addition, from the features that were used to create the target variable, feature 2 and feature 3 correlate with the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59dc222",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = features.copy()\n",
    "df['target'] = target\n",
    "plot_correlation_matrix(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c7c9e5",
   "metadata": {},
   "source": [
    "Let's now see how the correlation between feature 1, which was not used to compute the target variable, and feature 2 influences the Permutation Feature Importance! Therefore, we train a model on all features and calculate the feature importance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb3d8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into training and test data, then train model on training data\n",
    "x_train, x_test, y_train, y_test = train_test_split(features, target, random_state=seed, train_size=0.6)\n",
    "pipe.fit(x_train, y_train)\n",
    "\n",
    "# model performance on training and test data\n",
    "print('Model Performance on training data: {}'.format(pipe.score(x_train, y_train)))\n",
    "print('Model Performance on test data: {}'.format(pipe.score(x_test, y_test)))\n",
    "\n",
    "# Now back to obtaining importances\n",
    "explanation_train = permutation_importance(\n",
    "    pipe,\n",
    "    X=x_train,\n",
    "    y=y_train,\n",
    "    n_repeats=10,\n",
    "    random_state=seed,\n",
    "    scoring=scorer)\n",
    "\n",
    "explanation_train[\"feature\"] = x_train.columns\n",
    "\n",
    "plot_permutation_feature_importance(explanation_train, x_train, 'Permutation Feature Importance for Full Model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763e16b9",
   "metadata": {},
   "source": [
    "Next, we remove the highly correlated feature 1, retrain the model and calculate the feature importance. Removing feature 1 should not have a great impact on the model since it was not used to create the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a193608f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop correlated feature 1\n",
    "features.drop('feature_1', axis=1, inplace=True)\n",
    "\n",
    "# split data into training and test data, then train model on training data\n",
    "x_train, x_test, y_train, y_test = train_test_split(features, target, random_state=seed, train_size=0.6)\n",
    "pipe.fit(x_train, y_train)\n",
    "\n",
    "# model performance on training and test data\n",
    "print('Model Performance on training data: {}'.format(pipe.score(x_train, y_train)))\n",
    "print('Model Performance on test data: {}'.format(pipe.score(x_test, y_test)))\n",
    "\n",
    "# Now back to obtaining importances\n",
    "explanation_train = permutation_importance(\n",
    "    pipe,\n",
    "    X=x_train,\n",
    "    y=y_train,\n",
    "    n_repeats=10,\n",
    "    random_state=seed,\n",
    "    scoring=scorer)\n",
    "\n",
    "explanation_train[\"feature\"] = x_train.columns\n",
    "\n",
    "plot_permutation_feature_importance(explanation_train, x_train, 'Permutation Feature Importance for Reduced Model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21daa328",
   "metadata": {},
   "source": [
    "As can be seen from above, the model performance does not change when removing feature 1 from the dataset, which indicated that both models are comparable.  \n",
    "However, if we look at the feature importance of the full model, we can see that the importance is split between the correlated feature 1 and feature 2. Even though the data we simulated does not depend on feature 1, the model still assigns quite high importance to this feature as well, due to its high correlation with feature 2, which can lead to wrong conclusions during model interpretation! If we compare the feature importance of the full model to the one where we removed feature 1, we can see that the importance of feature 2 increases, although the importance of the remaining features stays the same. This illustrates why you have to be careful when using Permutation Feature Importance for models that were trained on datasets with high feature correlations!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f65eb61",
   "metadata": {},
   "source": [
    "\n",
    "<font color='green'>\n",
    "\n",
    "#### Question 1: What is achieved by permuting feature values?\n",
    "\n",
    "<font color='grey'>\n",
    "\n",
    "#### Your Answer: \n",
    "\n",
    "\n",
    "\n",
    "<font color='green'>\n",
    "\n",
    "#### Question 2: Would it also be a good idea to permute the target instead of the features?\n",
    "\n",
    "<font color='grey'>\n",
    "\n",
    "#### Your Answer: \n",
    "\n",
    "\n",
    "\n",
    "<font color='green'>\n",
    "\n",
    "#### Question 3: How should the output of permutation importance for a single feature be interpreted?\n",
    "\n",
    "<font color='grey'>\n",
    "\n",
    "#### Your Answer: \n",
    "\n",
    "\n",
    "\n",
    "<font color='green'>\n",
    "\n",
    "#### Question 4: What does a negative importance value mean?\n",
    "\n",
    "<font color='grey'>\n",
    "\n",
    "#### Your Answer: \n",
    "\n",
    "\n",
    "\n",
    "<font color='green'>\n",
    "\n",
    "#### Question 5: Why don't we leave out a feature instead of permuting it to measure its importance?\n",
    "\n",
    "<font color='grey'>\n",
    "\n",
    "#### Your Answer: \n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3.10.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
