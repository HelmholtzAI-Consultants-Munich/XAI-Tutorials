{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![logo](https://github.com/donatellacea/DL_tutorials/blob/main/notebooks/figures/1128-191-max.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XAI in Deep Learning-Based Signal Analysis: Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Notebook, we introduce the concept of transformers in machine learning, highlighting their significance in natural language processing (NLP) and Computer Vision (CV)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization is a fundamental step in text processing and natural language processing (NLP). It involves splitting text into smaller units, called tokens. Tokens are often words, but they can also be characters, subwords, or even sentences, depending on the level of tokenization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:\n",
    "Text: \"Natural Language Processing is fascinating.\"\n",
    "Tokens: [\"Natural\", \"Language\", \"Processing\", \"is\", \"fascinating\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1045,  2293,  4083,  2047,  2477,   999,   102,     0],\n",
      "        [  101,  2023, 14924,  4818,  2003,  3565,  4658,  1012,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "raw_inputs = [\n",
    "    \"I love learning new things!\",\n",
    "    \"This tutorial is super cool.\",\n",
    "]\n",
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embeddings in the context of machine learning are a type of representation that allows words, phrases, or even entire sentences to be transformed into vectors of real numbers. The dimensions of this vector represent different features of the word, capturing its various semantic and syntactic properties. Unlike one-hot encoding which treats each word as independent, embeddings can capture the context and relationships between words, making them much more informative.\n",
    "\n",
    "The concept of embedding is crucial because it provides a way to represent text in a format that machine learning algorithms can understand and process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
