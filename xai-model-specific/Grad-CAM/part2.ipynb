{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![logo](https://github.com/donatellacea/DL_tutorials/blob/main/notebooks/figures/1128-191-max.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XAI in deep learning-based image analysis \n",
    "\n",
    "## Part 2: Applying Grad-CAM\n",
    "\n",
    "---\n",
    "\n",
    "In the second part of this tutorial we are going to discover Grad-CAM, a gradient-based pixel attribution method for  understanding which parts of the image the model considers more relevant when making its prediction. We will again be using the pre-trained ResNet-50 and play around with various images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Colab environment\n",
    "\n",
    "If you installed the packages and requirements on your own machine, you can skip this section and start from the import section.\n",
    "Otherwise you can follow and execute the tutorial on your browser. In order to start working on the notebook, click on the following button, this will open this page in the Colab environment and you will be able to execute the code on your own.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/HelmholtzAI-Consultants-Munich/XAI-Tutorials/blob/ml4earth-Hackathon/xai-model-specific/Grad-CAM/part2.ipynb)\n",
    "\n",
    "Now that you are visualizing the notebook in Colab, run the next cell, in order to create a folder in your Google Drive. All the files for this tutorial will be uploaded to this folder. After the first execution you might receive some warning and notification, please follow these instructions:\n",
    "1. Warning: This notebook was not authored by Google. *Click* on 'Run anyway'.\n",
    "2. Permit this notebook to access your Google Drive files? *Click* on 'Yes', and select your account.\n",
    "3. Google Drive for desktop wants to access your Google Account. *Click* on 'Allow'.\n",
    "\n",
    "At this point, a folder has been created and you can navigate it through the lefthand panel in Colab, you might also have received an email that informs you about the access on your Google Drive. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a folder in your Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd drive/MyDrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't run this cell if you already cloned the repo in the first part of the tutorial\n",
    "!git clone https://github.com/HelmholtzAI-Consultants-Munich/XAI-Tutorials.git "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd XAI-Tutorials/xai-model-specific/Grad-CAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5GIj5Oyq1HwH"
   },
   "outputs": [],
   "source": [
    "# Installing the necessary packages\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "\n",
    "from tools.gradcam_model import GradCamModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grad-CAM: theoretical part\n",
    "\n",
    "Before starting with the hands-on part, click on the next image and watch a video that summarizes the theory behind the Grad-CAM method.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import VimeoVideo\n",
    "\n",
    "VimeoVideo(\"745319946?h=fcd327fc80\", width=800, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mTUk2DGa1X5_"
   },
   "source": [
    "## Grad-CAM: hands-on\n",
    "\n",
    "### Loading and preparing the input image\n",
    "\n",
    "We need to perform the following steps to have the input image ready to be fed to the model:\n",
    "\n",
    "* Load the image\n",
    "* Convert the image to RGB channel\n",
    "* Transform the image -\n",
    "  * Convert to PIL format\n",
    "  * Resize to lower resolution\n",
    "  * Convert to tensor dtype\n",
    "  * Normalize the pixel values\n",
    "* Unsqueeze the image to add a batch dimension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_img(path_to_img):\n",
    "    img = cv.imread(path_to_img) # Insert the path to image.\n",
    "    img = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n",
    "    return img\n",
    "\n",
    "# define the transformations (here we are using the ImageNet transformations)\n",
    "transform = transforms.Compose([transforms.ToPILImage(),\n",
    "                                transforms.Resize((224, 224)), \n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "def transform_img(img, transform):\n",
    "    arr_img = np.array(img)\n",
    "    # apply the transforms\n",
    "    trans_img = transform(arr_img)\n",
    "    # unsqueeze to add a batch dimension\n",
    "    trans_img = trans_img.unsqueeze(0)\n",
    "    return trans_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 305
    },
    "id": "CIzW8wu31YPV",
    "outputId": "b2171fb9-8c46-488f-f9f0-f1a29e77c793"
   },
   "outputs": [],
   "source": [
    "# read and visualize the image\n",
    "path_to_img = 'images/Iguana.jpeg'\n",
    "img = read_img(path_to_img)\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "\n",
    "# next transform the so it is ready to be given to the model\n",
    "trans_img = transform_img(img, transform)\n",
    "print('Transformed image size: ', trans_img.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AkBbIUWn2c-2"
   },
   "source": [
    "### Loading a Grad-Cam model\n",
    "\n",
    "For this tutorial, we are going to use the GradCamModel class which includes a pretrained ResNet-50 model and functions for returning the feature map activations and gradients. If you are interested you will find this class in ```gradcam_model.py```.\n",
    "\n",
    "Let's start loading the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init the GradCamModel\n",
    "gcmodel = GradCamModel()\n",
    "\n",
    "# set the evaluation mode\n",
    "_ = gcmodel.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward pass\n",
    "\n",
    "After instantiating GradCamModel, we make a forward pass through the network and see what the ResNet-50 predicts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward pass\n",
    "out = gcmodel(trans_img) \n",
    "pred = out.argmax(dim=1)\n",
    "\n",
    "print('Classification result:', pred.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oRp4DwKskOqT"
   },
   "source": [
    "Click [this link](https://deeplearning.cms.waikato.ac.nz/user-guide/class-maps/IMAGENET/) to identify the class name and see if the prediction is correct. \n",
    "\n",
    "Now that we verified that our model correctly classifies the iguana, we are going to do the back-propagation with the logit of the predicted class in the ImageNet dataset.\n",
    "\n",
    "Grad-CAM uses the feature maps produced by the last convolutional layer of a CNN. The authors of Grad-CAM argue, _“we can expect the last convolutional layers to have the best compromise between high-level semantics and detailed spatial information.”_\n",
    "\n",
    "Grad-CAM is applied to a neural network that has finished training. The weights of the neural network are fixed. We feed an image into the network to calculate the Grad-CAM heatmap for a specific image and a chosen class of interest.\n",
    "\n",
    "### Grad-CAM Details\n",
    "\n",
    "Let us assume $y^c$ is the score for class $c$ i.e., the output for class $c$ before the softmax.\n",
    "\n",
    "**Step 1: Computing Gradient**\n",
    "\n",
    "Compute the gradient of $y^c$ with respect to the feature map activation $A^k$ of a convolution layer i.e., $\\frac {∂y^c}{∂A^k}$\n",
    "\n",
    "**Step 2: Calculate Global Average Pooling (GAP) of the feature map.**\n",
    "     \n",
    "Global average pool the gradients over the width dimension (indexed by $i$) and the height dimension (indexed by $j$) to obtain weights ${\\alpha_k^c}$\n",
    "\n",
    "  $\n",
    "  {\\alpha_k^c} = \\frac {1}{Z} \\sum_{i} \\sum_{j} \\frac {∂y^c}{∂A^k_{ij}}\n",
    "  $\n",
    "\n",
    "**Step 3: Calculate Final Grad-CAM Localization Map**\n",
    "     \n",
    "Perform a weighted combination of the feature map activations $A^k$ where the weights are the ${\\alpha_k^c}$ we just calculated and keep only positive contributions applying a ReLU function.\n",
    "\n",
    "  $\n",
    "  L^c_{Grad-CAM} = ReLU (\\sum_k {\\alpha_k^c} A^k)\n",
    "  $\n",
    "\n",
    "#### Exercise 1\n",
    "Follow the steps described above and change the **Nones** with your code to complete the Grad-CAM algorithm.\n",
    "\n",
    "HINTS: \n",
    "\n",
    "1. Don't forget the batch dimension - gradients will have a size of $[Batch \\times k \\times i \\times j]$\n",
    "2. Use numpy to apply the ReLU function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_localization_map(gcmodel, img, out, c):\n",
    "    \n",
    "    # Step 1 - Gradient output y wrt. to activation map \n",
    "    # get the gradient of the output with respect to the parameters of the model\n",
    "    out[:,c].backward(retain_graph=True)\n",
    "    # pull the gradients out of the model\n",
    "    gradients = gcmodel.get_gradient()\n",
    "\n",
    "    # Step 2 - Global average pooling\n",
    "    # pool the gradients across the channels\n",
    "    pooled_gradients = torch.mean(None, dim=None)\n",
    "\n",
    "    # Step 3 - Weighted combination of influence and feature maps\n",
    "    # get the activations of the last convolutional layer\n",
    "    activations = gcmodel.get_activations(img).detach()    \n",
    "    # weight the channels by corresponding gradients\n",
    "    for i in range(activations.size(1)):\n",
    "        activations[:, i, :, :] *= pooled_gradients[i]\n",
    "    # average the channels of the activations\n",
    "    localization_map = torch.sum(activations, dim=1).squeeze()\n",
    "    # convert the map to be a numpy array\n",
    "    localization_map = localization_map.numpy()\n",
    "    # relu on top of the localization map\n",
    "    localization_map = None\n",
    "    \n",
    "    return localization_map\n",
    "\n",
    "def convert_to_heatmap(localization_map, img):\n",
    "    # normalize the localization_map\n",
    "    localization_map /= np.max(localization_map)\n",
    "    # resize to image size\n",
    "    heatmap = cv.resize(localization_map, (img.shape[1], img.shape[0]))\n",
    "    # normalize to [0, 255] range and convert to unsigned int\n",
    "    heatmap = np.uint8(255 * heatmap)\n",
    "    return heatmap\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmap\n",
    "\n",
    "Call calculate_heatmap to get the Res-Net GradCAM output for class 'iguana'\n",
    "\n",
    "Next, process the result for visualisation and visualise it along with the original image and the Grad-CAM heat-map projected onto the original image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "localization_map = calculate_localization_map(gcmodel, trans_img, out, c=39)\n",
    "\n",
    "heatmap = convert_to_heatmap(localization_map, img)\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.subplot(131)\n",
    "plt.imshow(img)\n",
    "plt.title('Original image')\n",
    "plt.subplot(132)\n",
    "plt.imshow(localization_map.squeeze())\n",
    "plt.title('Heat-Map for the image')\n",
    "plt.subplot(133)\n",
    "plt.imshow(heatmap, cmap='jet', alpha=0.5)\n",
    "plt.imshow(img, alpha=0.5)\n",
    "plt.title('Grad-CAM heat-map projected onto the original image')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's look at a different image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read and visualize an image\n",
    "img = read_img('images/cat_and_dog.jpg')\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "\n",
    "# next transform it so it is ready to be given to the model\n",
    "trans_img = transform_img(img, transform)\n",
    "print('Transformed image size: ', trans_img.size())\n",
    "\n",
    "# forward pass\n",
    "out = gcmodel(trans_img) # img)\n",
    "pred = out.argmax(dim=1)\n",
    "\n",
    "print('Classification result:', pred.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the model predicting? Let's look at the top two predictions and try and understand each!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_sort= torch.argsort(out, descending=True)\n",
    "top_ten = pred_sort[0,:10]\n",
    "print('Top ten probabilities: ', top_ten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a moment to identify the class names [ImageNet class list](https://deeplearning.cms.waikato.ac.nz/user-guide/class-maps/IMAGENET/).\n",
    "\n",
    "Now let's use GradCAM to try and understand where the model is focusing for specific probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_cat = calculate_localization_map(gcmodel, trans_img, out, c=282)\n",
    "heatmap_dog = calculate_localization_map(gcmodel, trans_img, out, c=231)\n",
    "\n",
    "heatmap_catn = convert_to_heatmap(heatmap_cat, img)\n",
    "heatmap_dogn = convert_to_heatmap(heatmap_dog, img)\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.subplot(141)\n",
    "plt.imshow(heatmap_cat.squeeze())\n",
    "plt.title('Heatmap for class  garden cat')\n",
    "plt.subplot(142)\n",
    "plt.imshow(heatmap_catn, cmap='jet', alpha=0.4)\n",
    "plt.imshow(img, alpha=0.6)\n",
    "plt.title('Grad-CAM cat projection')\n",
    "plt.subplot(143)\n",
    "plt.imshow(heatmap_dog.squeeze())\n",
    "plt.title('Heatmap for class  dog')\n",
    "plt.subplot(144)\n",
    "plt.imshow(heatmap_dogn, cmap='jet', alpha=0.4)\n",
    "plt.imshow(img, alpha=0.6)\n",
    "plt.title('Grad-CAM dog projection')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Congratulations! You completed this tutorial.\n",
    "\n",
    "## Questionnaire\n",
    "What did we learn today?\n",
    "\n",
    "Look at [this short questionnaire](https://docs.google.com/presentation/d/1vd_HfkBD4FokoAM2el4T1rXWnD7_0FAEvxk4HgiYjXs/present?slide=id.g1499f1249e3_0_0) to revise what you have learned in our session today!\n",
    "\n",
    "## Homework\n",
    "\n",
    "* Look at the top-10 classes of the model - can you see a pattern?\n",
    "* Can you implement the occlusion technique, see [here](https://towardsdatascience.com/inshort-occlusion-analysis-for-explaining-dnns-d0ad3af9aeb6), by removing the areas of the dog and cat on the last image and see what you get?\n",
    "\n",
    "## Further reading:\n",
    "\n",
    "Here is the original GradCAM paper: https://arxiv.org/pdf/1610.02391.pdf\n",
    "\n",
    "This tutorial was based on: https://medium.com/@stepanulyanin/grad-cam-for-resnet152-network-784a1d65f3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "XplainableAITutorial2V2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
