{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![logo](https://github.com/donatellacea/DL_tutorials/blob/main/notebooks/figures/1128-191-max.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XAI in deep learning-based image analysis \n",
    "\n",
    "## Part 1: Feature Visualisation\n",
    "\n",
    "---\n",
    "\n",
    "In the first part of this tutorial we will see some examples of how to visualise deep learning filters (kernels) and feature maps.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Colab environment\n",
    "\n",
    "If you installed the packages and requirements on your own machine, you can skip this section and start from the import section.\n",
    "Otherwise you can follow and execute the tutorial on your browser. In order to start working on the notebook, click on the following button, this will open this page in the Colab environment and you will be able to execute the code on your own.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/HelmholtzAI-Consultants-Munich/XAI-Tutorials/blob/ml4earth-Hackathon/xai-model-specific/Grad-CAM/part1.ipynb)\n",
    "\n",
    "Now that you are visualizing the notebook in Colab, run the next cell, in order to create a folder in your Google Drive. All the files for this tutorial will be uploaded to this folder. After the first execution you might receive some warning and notification, please follow these instructions:\n",
    "1. Warning: This notebook was not authored by Google. *Click* on 'Run anyway'.\n",
    "2. Permit this notebook to access your Google Drive files? *Click* on 'Yes', and select your account.\n",
    "3. Google Drive for desktop wants to access your Google Account. *Click* on 'Allow'.\n",
    "\n",
    "At this point, a folder has been created and you can navigate it through the lefthand panel in Colab, you might also have received an email that informs you about the access on your Google Drive. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a folder in your Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd drive/MyDrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't run this cell if you already cloned the repo in the first part of the tutorial\n",
    "!git clone https://github.com/HelmholtzAI-Consultants-Munich/XAI-Tutorials.git "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd XAI-Tutorials/xai-model-specific/Grad-CAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2YFsRsKbRc8T"
   },
   "outputs": [],
   "source": [
    "# Installing the necessary packages\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import cv2 as cv\n",
    "import argparse\n",
    "from torchvision import models, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature visualization: theoretical part\n",
    "\n",
    "Before starting with the hands-on part, click on the following image and watch a video that summarizes the concepts of Convolutional Neural Network (CNN) and feature visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import VimeoVideo\n",
    "\n",
    "VimeoVideo(\"745320494?h=0b8be077b3\", width=800, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature visualization: hands-on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model: Res-Net50\n",
    "\n",
    "Throughout this tutorial we will be using the ResNet-50 model. Take a minute now to look at the [model's architecture](https://www.mdpi.com/metals/metals-11-00388/article_deploy/html/images/metals-11-00388-g004.png).\n",
    "\n",
    "*Find out more:* [here](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7780459) you can read the original paper for the ResNet-50 model (He, Kaiming, et al., 2016)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X3CGo2rnEePB"
   },
   "source": [
    "Running the next cell you will:\n",
    "* load the pre-trained ImageNet weights. This will allow us to get better visualizations without training the convolutional neural network.\n",
    "* print the model, take a minute to look at its structure\n",
    "* create the empty lists **model_weights** and **conv_layers**, where we will save weghts of the convolutional layers and the convolutional layers' names. It is important to remember that the ResNet-50 model has 50 layers in total: 49 of those which are convolutional layers and the final is a fully connected layer.\n",
    "* call the model.children() module, i.e. a generator that returns layers of the model from which you can extract your parameter tensors using \"layername.weight\" or \"layername.bias\" and is saved as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "0aeb4febfe8a43daa1b0ec3ecf0cf18f",
      "7bd21c7f56bc48ffbce204494702dc25",
      "d6339e55e877416790797385b0fe5747",
      "f9834c2ff3ed4ee2a084ee3e84d5dbe6",
      "dbf9c3be31194a68bb1295799beea2f7",
      "2a4e98c765cd459ab722d93424869191",
      "77ff2791517a421b972b3fa4ad0e4d59",
      "975c55d7784642b290359ac7e3026ecf",
      "e29bd7a6a1cc462e8bfdf54d75e555db",
      "056f6a47bf1d46bbbb55ec2e313f7c86",
      "cb63bc11ef1f464884af8b030057d2b8"
     ]
    },
    "id": "QtHBjIC_DQlU",
    "outputId": "66e5aade-313f-4c9e-e91c-fa8fbdcea89a"
   },
   "outputs": [],
   "source": [
    "# load the model\n",
    "model = models.resnet50(pretrained=True)\n",
    "print(model)\n",
    "model_weights = [] # we will save the conv layer weights in this list\n",
    "conv_layers = [] # we will save the 49 conv layers in this list\n",
    "# get all the model children as list\n",
    "model_children = list(model.children())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "km0H9NONaRhd"
   },
   "source": [
    "At certain stages of the model, there are nestings of Bottleneck layers within different layers. Also, the ResNet architecture has many hidden and sequential layers, so to keep only the convolutional layers we check that the two conditions below are satisfied. Then we append the child node and weights to the conv_layers and model_weights respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LhQ2kGmODQ-z",
    "outputId": "b964697f-f2a0-484c-ef66-199dd1d21e7b"
   },
   "outputs": [],
   "source": [
    "# counter to keep count of the conv layers\n",
    "counter = 0 \n",
    "# append all the conv layers and their respective weights to the list\n",
    "for i in range(len(model_children)):\n",
    "    if type(model_children[i]) == nn.Conv2d:\n",
    "        counter += 1\n",
    "        model_weights.append(model_children[i].weight)\n",
    "        conv_layers.append(model_children[i])\n",
    "    elif type(model_children[i]) == nn.Sequential:\n",
    "        for j in range(len(model_children[i])):\n",
    "            for child in model_children[i][j].children():\n",
    "                if type(child) == nn.Conv2d:\n",
    "                    counter += 1\n",
    "                    model_weights.append(child.weight)\n",
    "                    conv_layers.append(child)\n",
    "print(f\"Total convolutional layers: {counter}\")\n",
    "first_conv_size = model_weights[0].shape\n",
    "print('Shape of Model weights of 1st Conv2d Layer is', first_conv_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3km6BQYHc9mw"
   },
   "source": [
    "### Filter visualization in the first layer\n",
    "\n",
    "For visualizing the first conv layer filters, we are iterating through the weights of the first convolutional layer.\n",
    "The output is going to be 64 filters of 7×7 dimensions. The 64 refers to the number of hidden units in that layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 955
    },
    "id": "Iciw5qaVDRHp",
    "outputId": "e45e5ea2-96ca-4e10-af31-709e2a6ec004"
   },
   "outputs": [],
   "source": [
    "# visualize the model weights of the first conv layer\n",
    "\n",
    "plt.figure(figsize=(20, 17))\n",
    "for i, filter in enumerate(model_weights[0]):\n",
    "    # get the filter you want to visualise\n",
    "    plt.subplot(8, 8, i+1) # the number of subplots should be as many as the filters we want to visualise \n",
    "    plt.imshow(filter[0, :, :].detach(), cmap='gray')\n",
    "    plt.axis('off')\n",
    "    # plt.savefig('../outputs/filter.png') # Optional\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5LmoGW9WRc8X"
   },
   "source": [
    "We see that diffirent kernels are focusing on different shapes, e.g. diagonal lines, squares etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6cHmMZrCkWXm"
   },
   "source": [
    "### The input image: loading and preparing for the model\n",
    "\n",
    "Now that we are familiar with the model and the filters, we can see how more and more complex features are learnt in different layers. Let's start choosing an image and preparing it following the next steps:\n",
    "\n",
    "* We first read the image by specifying the path to image.\n",
    "* We define the image transformation which first converts the image to PIL format, resizes to a standard dimension of 512x512, changes the dtype to torch tensor, and normalises the image to the 0-1 range.\n",
    "* Finally, we add the batch dimension to the existing image dimension \n",
    "\n",
    "*Note that DL models are trained in batches of images but in our case we have only one image so we add a batch dimension of size 1 - so there is only one image in this batch. Now the size of the image, instead of being [3, 512, 512], is [1, 3, 512, 512], indicating that there is only one image in the batch.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 304
    },
    "id": "WoxDkZwVLWfz",
    "outputId": "2459cb5c-2dfa-4ec6-88c9-896c6aaafb23"
   },
   "outputs": [],
   "source": [
    "# read and visualize an image\n",
    "img = cv.imread(\"images/cat-g251f891e8_1920.jpg\") # Insert the path to image.\n",
    "img = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "# define the transforms (here we are using the ImageNet transformations)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((512, 512)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "img = np.array(img)\n",
    "# apply the transforms\n",
    "img = transform(img)\n",
    "print(img.size())\n",
    "# unsqueeze to add a batch dimension\n",
    "img = img.unsqueeze(0)\n",
    "print(img.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QvpyNht8lKz3"
   },
   "source": [
    "### Feature Maps of the Convolutional Layers\n",
    "\n",
    "Visualizing the feature maps of the image after passing through the convolutional layers of the ResNet-50 model consists of two steps:\n",
    "\n",
    "  * Passing the image through each convolutional layer and saving each layer’s output.\n",
    "  * Visualizing the feature map blocks of each layer.\n",
    "\n",
    "We will first give the image as an input to the first convolutional layer. After that, we will use a for loop to pass the last layer’s outputs to the next layer, until we reach the last convolutional layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass the image through first layer\n",
    "results = [conv_layers[0] (img)]\n",
    "for i in range(1, len(conv_layers)):\n",
    "    # pass the result from the last layer to the next layer\n",
    "    results.append(conv_layers[i](results[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PkrtR0g6pivU"
   },
   "source": [
    "### Visualizing the Feature Maps\n",
    "\n",
    "The final layers (near the final fully connected layer) have many feature maps, in the range of 512 to 2048 _(again: look at the model architecture!)_. We will only visualize 64 feature maps from a layer at the beginning of the model, in the middle and at the final convolutional layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "1Nm6Zd8Xq-uL",
    "outputId": "5abadb9e-57a5-4f60-81ea-1d3391db554b"
   },
   "outputs": [],
   "source": [
    "num_layer = [0,24,48]\n",
    "for layer in num_layer:\n",
    "\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    layer_viz = results[layer][0, :, :, :]\n",
    "    layer_viz = layer_viz.data\n",
    "    print(f'Size of layer {layer+1} is {layer_viz.size()}')\n",
    "    print(f'Feature maps from the first convolutional layer (layer {num_layer}) of ResNet-50 model')\n",
    "    for i, filter in enumerate(layer_viz):\n",
    "        if i == 64: # we will visualize only 8x8 blocks from each layer\n",
    "            break\n",
    "        plt.subplot(8, 8, i + 1)\n",
    "        plt.imshow(filter, cmap='gray')\n",
    "        plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fhIvw8J1-tls"
   },
   "source": [
    "## Homework\n",
    "\n",
    "1. Play around with plotting pretrained filters in different layers and get a high level understanding of the region each filter concentrates on in an image as you move up the convolutional layers.\n",
    "\n",
    "2. Note that in the feature visualisation of the first layer we are only looking at a third of the features as we have size 3 in dimension 1. How do the other features look like? \n",
    "\n",
    "3. Similarly, feel free to explore the feature maps at different layers and try to understand the evolution of feature maps and depth of the image in recognizing features that are important for classification.\n",
    "\n",
    "4. Think about using a non trained network, that has no pretrained weights and pass the image through the network. Would you find any difference?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P4S5eW_4Oh9x"
   },
   "source": [
    "## References \n",
    "\n",
    "This tutorial was based on:  https://debuggercafe.com/visualizing-filters-and-feature-maps-in-convolutional-neural-networks-using-pytorch/\n",
    "\n",
    "Theoretical points were taken from: https://christophm.github.io/interpretable-ml-book/neural-networks.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "XplainableAITutorial1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "056f6a47bf1d46bbbb55ec2e313f7c86": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0aeb4febfe8a43daa1b0ec3ecf0cf18f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7bd21c7f56bc48ffbce204494702dc25",
       "IPY_MODEL_d6339e55e877416790797385b0fe5747",
       "IPY_MODEL_f9834c2ff3ed4ee2a084ee3e84d5dbe6"
      ],
      "layout": "IPY_MODEL_dbf9c3be31194a68bb1295799beea2f7"
     }
    },
    "2a4e98c765cd459ab722d93424869191": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "77ff2791517a421b972b3fa4ad0e4d59": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7bd21c7f56bc48ffbce204494702dc25": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2a4e98c765cd459ab722d93424869191",
      "placeholder": "​",
      "style": "IPY_MODEL_77ff2791517a421b972b3fa4ad0e4d59",
      "value": "100%"
     }
    },
    "975c55d7784642b290359ac7e3026ecf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cb63bc11ef1f464884af8b030057d2b8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d6339e55e877416790797385b0fe5747": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_975c55d7784642b290359ac7e3026ecf",
      "max": 102530333,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e29bd7a6a1cc462e8bfdf54d75e555db",
      "value": 102530333
     }
    },
    "dbf9c3be31194a68bb1295799beea2f7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e29bd7a6a1cc462e8bfdf54d75e555db": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f9834c2ff3ed4ee2a084ee3e84d5dbe6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_056f6a47bf1d46bbbb55ec2e313f7c86",
      "placeholder": "​",
      "style": "IPY_MODEL_cb63bc11ef1f464884af8b030057d2b8",
      "value": " 97.8M/97.8M [00:00&lt;00:00, 203MB/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
